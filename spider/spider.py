#!/usr/ebin/env python

import requests
import re
import urllib.parse as urlparse

ulinks = []
target_url = "http://192.168.5.132/mutillidae/"


def extract_links(url):
    response = requests.get(url).content
    return re.findall('(?:\shref=")(.*?)"', str(response))


def crawl(target):
    links = extract_links(target)
    for link in links:
        link = urlparse.urljoin(target, link)
        if target in link and link not in ulinks:
            ulinks.append(link)
            crawl(link)


crawl(target_url)
for link in ulinks:
    print(link + "\n")
